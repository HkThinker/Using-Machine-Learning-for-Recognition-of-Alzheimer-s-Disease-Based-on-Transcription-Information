{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_ad_dir = '../train/transcription/cd/*'\n",
    "controls_dir = '../train/transcription/cc/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(file_name):\n",
    "    par = {}\n",
    "    par['id'] = file_name.split('/')[-1].split('.cha')[0]\n",
    "    f = iter(open(file_name))\n",
    "    l = next(f)\n",
    "    speech = []\n",
    "    try:\n",
    "        curr_speech = ''\n",
    "        while (True):\n",
    "            if l.startswith('*PAR:') or l.startswith('*INV'):\n",
    "                curr_speech = l\n",
    "            elif len(curr_speech) != 0 and not(l.startswith('%') or l.startswith('*')):\n",
    "                curr_speech += l\n",
    "            elif len(curr_speech) > 0:\n",
    "                speech.append(curr_speech)\n",
    "                curr_speech = ''\n",
    "            l = next(f)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "    clean_par_speech = []\n",
    "    clean_all_speech = []\n",
    "    is_par = False\n",
    "    for s in speech:\n",
    "        \n",
    "        def _clean(s):\n",
    "            s = re.sub('\\x15\\d*_\\d*\\x15', '', s) # remove time block \n",
    "            s = re.sub('\\[.*\\]', '', s) # remove other speech artifacts [.*]\n",
    "            s = s.strip()\n",
    "            s = re.sub('\\t|\\n|<|>', '', s) # remove tab, new lines,ampersand\n",
    "            return s\n",
    "        \n",
    "        if s.startswith('*PAR:'):\n",
    "            is_par = True\n",
    "        elif s.startswith('*INV:'):\n",
    "            is_par = False\n",
    "            s = re.sub('\\*INV:\\t', '', s) # remove prefix\n",
    "        if is_par:\n",
    "            s = re.sub('\\*PAR:\\t', '', s) # remove prefix    \n",
    "            clean_par_speech.append(_clean(s))\n",
    "        clean_all_speech.append(_clean(s))\n",
    "        \n",
    "    par['speech'] = speech\n",
    "    par['clean_speech'] = clean_all_speech\n",
    "    par['clean_par_speech'] = clean_par_speech\n",
    "    par['joined_all_speech'] = ' '.join(clean_all_speech)\n",
    "    par['joined_all_par_speech'] = ' '.join(clean_par_speech)\n",
    "    \n",
    "    return par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_train_data():\n",
    "    return _parse_data('../data/train')\n",
    "\n",
    "def _parse_data(data_dir):\n",
    "    prob_ad_dir = f'{data_dir}/transcription/cd/*'\n",
    "    controls_dir = f'{data_dir}/transcription/cc/*'\n",
    "    \n",
    "    prob_ad = [extract_data(fn) for fn in glob(prob_ad_dir)]\n",
    "    controls = [extract_data(fn) for fn in glob(controls_dir)]\n",
    "    controls_df = pd.DataFrame(controls)\n",
    "    prob_ad_df = pd.DataFrame(prob_ad)\n",
    "    controls_df['ad'] = 0\n",
    "    prob_ad_df['ad'] = 1\n",
    "    df = pd.concat([controls_df, prob_ad_df]).sample(frac=1).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = parse_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomforst_models(text: pd.Series, labels: list, shuffle=True):\n",
    "    ## AD Classification Pred\n",
    "    \n",
    "    # sklearn pipeline\n",
    "    param_space = {\n",
    "        'vec__max_features': [100, 500, 1000, 2000, 10000],\n",
    "        'vec__stop_words': ['english', None],\n",
    "        'vec__analyzer': ['word', 'char'],\n",
    "        'vec__max_df': [0.5, 0.75, 1.0],\n",
    "        'vec__sublinear_tf': [True, False]    \n",
    "     \n",
    "    }    \n",
    "    param_space['clf__n_estimators'] = [10]\n",
    "    param_space['clf__max_depth'] = [5, 10, 15]\n",
    "    param_space['clf__min_samples_split']= [2, 5]\n",
    "    param_space['clf__min_samples_leaf']= [1, 2, 4]\n",
    "    param_space ['clf__bootstrap']= [True, False]\n",
    "\n",
    "    clf_pipe = Pipeline([\n",
    "        ('vec', TfidfVectorizer()),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(text, labels, random_state=random_state, test_size=0.2,shuffle=shuffle)\n",
    "    search = GridSearchCV(clf_pipe, param_space, cv=10, n_jobs=6)\n",
    "    search.fit(train_features, train_labels)\n",
    "    clf_pipe.set_params(**search.best_params_)\n",
    "    print(search.best_params_)\n",
    "    clf_pipe.fit(train_features, train_labels)\n",
    "  \n",
    "    return clf_pipe, test_features,test_labels,train_features,train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ran_par,test_features,test_labels,train_features,train_labels = randomforst_models(train_df.joined_all_par_speech, train_df.ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# define Bootstrap Sampling method\n",
    "def bootstrap_sampling(text: pd.Series, labels: list, shuffle=True,n_iterations=5):\n",
    "    stats = []\n",
    "    ## AD Classification Pred\n",
    "    # sklearn pipeline\n",
    "    param_space = {\n",
    "        'vec__max_features': [100, 500, 1000, 2000, 10000],\n",
    "        'vec__stop_words': ['english', None],\n",
    "        'vec__analyzer': ['word', 'char'],\n",
    "        'vec__max_df': [0.5, 0.75, 1.0],\n",
    "        'vec__sublinear_tf': [True, False]    \n",
    "     \n",
    "    }    \n",
    "    param_space['clf__n_estimators'] = [10]\n",
    "    param_space['clf__max_depth'] = [5, 10, 15]\n",
    "    param_space['clf__min_samples_split']= [2, 5]\n",
    "    param_space['clf__min_samples_leaf']= [1, 2, 4]\n",
    "    param_space ['clf__bootstrap']= [True, False]\n",
    "\n",
    "    clf_pipe = Pipeline([\n",
    "        ('vec', TfidfVectorizer()),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        # Sampling with putback from the training set to generate a new training set\n",
    "        # Extracting features and labels\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(text, labels, test_size=0.2,shuffle=shuffle)\n",
    "        search = GridSearchCV(clf_pipe, param_space, cv=10, n_jobs=6)\n",
    "        search.fit(train_features, train_labels)\n",
    "        clf_pipe.set_params(**search.best_params_)\n",
    "        print(search.best_params_)\n",
    "        clf_pipe.fit(train_features, train_labels)\n",
    "\n",
    "        # Predictions on the test set\n",
    "        predictions = clf_pipe.predict(test_features)\n",
    "\n",
    "        # Calculation accuracy\n",
    "        accuracy = accuracy_score(test_labels, predictions)\n",
    "        stats.append(accuracy)\n",
    "    return stats\n",
    "\n",
    "# call Bootstrap Sampling to get the performance evaluation results\n",
    "bootstrap_stats = bootstrap_sampling(train_df.joined_all_par_speech, train_df.ad)\n",
    "mean_accuracy = np.mean(bootstrap_stats)\n",
    "std_accuracy = np.std(bootstrap_stats)\n",
    "\n",
    "print(\"Bootstrap Sampling method:\")\n",
    "print(f\"Average accuracy:{mean_accuracy:.3f}\")\n",
    "print(f\"Standard deviation of accuracy:{std_accuracy:.3f}\")\n",
    "\n",
    "confidence_interval = 0.95 \n",
    "alpha = (1 - confidence_interval) / 2\n",
    "\n",
    "# Sort the accuracy scores obtained from Bootstrap Sampling\n",
    "sorted_bootstrap_stats = sorted(bootstrap_stats)\n",
    "\n",
    "# Calculate the lower and upper percentiles to get the confidence interval\n",
    "lower_percentile_idx = int(len(sorted_bootstrap_stats) * alpha)\n",
    "upper_percentile_idx = int(len(sorted_bootstrap_stats) * (1 - alpha))\n",
    "\n",
    "lower_bound = sorted_bootstrap_stats[lower_percentile_idx]\n",
    "upper_bound = sorted_bootstrap_stats[upper_percentile_idx]\n",
    "\n",
    "print(f\"{confidence_interval * 100:.1f}% Confidence Interval:\")\n",
    "print(f\"Lower Bound: {lower_bound:.3f}\")\n",
    "print(f\"Upper Bound: {upper_bound:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# define K-Fold Cross Validation method\n",
    "def kfold_cross_validation(clf, features, labels, n_splits=10):\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(clf, features, labels, cv=kfold)\n",
    "    return scores\n",
    "\n",
    "# call K-Fold Cross Validation method to get the performance evaluation results\n",
    "kfold_scores = kfold_cross_validation(clf_ran_par, train_features, train_labels)\n",
    "mean_accuracy_kfold = np.mean(kfold_scores)\n",
    "std_accuracy_kfold = np.std(kfold_scores)\n",
    "print(\"K-Fold Cross Validation method:\")\n",
    "print(f\"Average accuracy:{mean_accuracy_kfold:.3f}\")\n",
    "print(f\"Standard deviation of accuracy:{std_accuracy_kfold:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LOSOCV \n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# define Leave One Subject Out Cross Validation method\n",
    "def leave_one_subject_out_cross_validation(clf, features, labels, subject_ids):\n",
    "    losocv = LeaveOneGroupOut()\n",
    "    scores = cross_val_score(clf, features, labels, groups=subject_ids, cv=losocv)  \n",
    "    return scores\n",
    "\n",
    "# call Leave One Subject Out Cross Validation method to get the performance evaluation results\n",
    "loso_cv_scores = leave_one_subject_out_cross_validation(clf_ran_par, train_df.joined_all_par_speech, train_df.ad, train_df.id)\n",
    "mean_accuracy_loso_cv = np.mean(loso_cv_scores)\n",
    "std_accuracy_loso_cv = np.std(loso_cv_scores)\n",
    "print(\"Leave One Subject Out Cross Validation method:\")\n",
    "print(f\"Average accuracy:{mean_accuracy_loso_cv:.3f}\")\n",
    "print(f\"Standard deviation of accuracy:{std_accuracy_loso_cv:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
